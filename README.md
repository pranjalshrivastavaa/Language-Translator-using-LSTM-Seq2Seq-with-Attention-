This project implements a Neural Machine Translation (NMT) system using LSTM-based Seq2Seq models with Attention to translate text between languages. It includes an encoder-decoder architecture, attention mechanism, custom preprocessing, and training pipeline using TensorFlow/Keras. The dataset is preprocessed and stored in the data/ directory, while the trained models are saved in models/.
Supports parallel text datasets for translation.
Implements Attention for better translation accuracy.
Future improvements: multilingual support and transformer models.
